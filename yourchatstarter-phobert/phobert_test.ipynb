{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"phobert_test.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1x7zvQBjTPurG524JCKATIp3iaqZtQRkZ","authorship_tag":"ABX9TyOn8plIHxuTMO1FzHI/wNrh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["install vncorenlp wrapper and transformer from pip"],"metadata":{"id":"1V9nc_gCofyM"}},{"cell_type":"code","source":["!pip3 install vncorenlp\n","!pip3 install transformers"],"metadata":{"id":"tKgHg3fWEgiJ","executionInfo":{"status":"ok","timestamp":1653004999620,"user_tz":-420,"elapsed":12031,"user":{"displayName":"Dang Nguyen Ngoc","userId":"11023288777081019836"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"46cd479f-8384-4e7d-bcfa-ea6f0576ea6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vncorenlp\n","  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2021.10.8)\n","Building wheels for collected packages: vncorenlp\n","  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645951 sha256=82cf6bd77b3e5e8813726cdf4fc4a234f6226be29b7f38035e7cf35914013715\n","  Stored in directory: /root/.cache/pip/wheels/0c/d8/f2/d28d97379b4f6479bf51247c8dfd57fa00932fa7a74b6aab29\n","Successfully built vncorenlp\n","Installing collected packages: vncorenlp\n","Successfully installed vncorenlp-1.0.3\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 52.9 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 53.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"]}]},{"cell_type":"markdown","source":["install VnCoreNLP (only the word-segmenter)"],"metadata":{"id":"cc1mVba7ooih"}},{"cell_type":"code","source":["!mkdir -p VnCoreNLP/models/wordsegmenter\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","!mv VnCoreNLP-1.1.1.jar VnCoreNLP/ \n","!mv vi-vocab VnCoreNLP/models/wordsegmenter/\n","!mv wordsegmenter.rdr VnCoreNLP/models/wordsegmenter/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJrImSmfG8r2","executionInfo":{"status":"ok","timestamp":1653005001393,"user_tz":-420,"elapsed":1789,"user":{"displayName":"Dang Nguyen Ngoc","userId":"11023288777081019836"}},"outputId":"32177718-94eb-4fb7-c9ff-46de4a4e303c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-20 00:03:20--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 27412575 (26M) [application/octet-stream]\n","Saving to: ‘VnCoreNLP-1.1.1.jar’\n","\n","VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  --.-KB/s    in 0.1s    \n","\n","2022-05-20 00:03:21 (175 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n","\n","--2022-05-20 00:03:21--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 526544 (514K) [application/octet-stream]\n","Saving to: ‘vi-vocab’\n","\n","vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.04s   \n","\n","2022-05-20 00:03:21 (14.3 MB/s) - ‘vi-vocab’ saved [526544/526544]\n","\n","--2022-05-20 00:03:21--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 128508 (125K) [text/plain]\n","Saving to: ‘wordsegmenter.rdr’\n","\n","wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n","\n","2022-05-20 00:03:21 (5.96 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n","\n"]}]},{"cell_type":"markdown","source":["download PhoBERT base model"],"metadata":{"id":"t_6PinanoxrD"}},{"cell_type":"code","source":["!git lfs install\n","!git clone https://huggingface.co/vinai/phobert-base"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1SapihQGmpJ","executionInfo":{"status":"ok","timestamp":1653005028107,"user_tz":-420,"elapsed":26720,"user":{"displayName":"Dang Nguyen Ngoc","userId":"11023288777081019836"}},"outputId":"c9f1b45f-70fd-4a35-c794-26ecb25e0790"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n","Git LFS initialized.\n","Cloning into 'phobert-base'...\n","remote: Enumerating objects: 42, done.\u001b[K\n","remote: Counting objects: 100% (42/42), done.\u001b[K\n","remote: Compressing objects: 100% (41/41), done.\u001b[K\n","remote: Total 42 (delta 16), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (42/42), done.\n","Filtering content: 100% (3/3), 1.69 GiB | 69.42 MiB/s, done.\n"]}]},{"cell_type":"markdown","source":["make training output folder"],"metadata":{"id":"XIO-eIffo3nh"}},{"cell_type":"code","source":["!mkdir training_output"],"metadata":{"id":"mbXUT_Z0IBrY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TRAINING MONTAGE!!!!"],"metadata":{"id":"8D1ECyL2o6-N"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LpgbzUusAhLl","outputId":"5e111dc7-5196-4808-d3d0-a47d493bbcb9","executionInfo":{"status":"ok","timestamp":1653013381954,"user_tz":-420,"elapsed":8353862,"user":{"displayName":"Dang Nguyen Ngoc","userId":"11023288777081019836"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Bạn muốn train model mới (y/n): y\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["Loading csv file\n"]},{"output_type":"stream","name":"stderr","text":["28574it [01:13, 389.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Encoding text data to input id\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28574/28574 [00:04<00:00, 5992.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Padding ids\n","Creating mask\n","Converting to torch tensor\n","Create data loader instance\n","len dataloader: 3572\n","All data loaded\n","Loading csv file\n"]},{"output_type":"stream","name":"stderr","text":["3183it [00:11, 273.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Encoding text data to input id\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3183/3183 [00:00<00:00, 6817.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Padding ids\n","Creating mask\n","Converting to torch tensor\n","Create data loader instance\n","len dataloader: 398\n","All data loaded\n","Loading pre-trained PhoBERT model\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at ./phobert-base/pytorch_model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./phobert-base/pytorch_model.bin and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Load weight from file:training_output\n","No checkpoint found in: training_output\n","Check save_dir...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["======== Epoch 1 / 10 ========\n","Training...\n","[TRAIN] Epoch 1/10 | Batch 0/3572 | Train Loss=1.0610910654067993 | Train Acc=0.5\n","[TRAIN] Epoch 1/10 | Batch 100/3572 | Train Loss=0.5458143353462219 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 200/3572 | Train Loss=0.5000765323638916 | Train Acc=0.875\n","[TRAIN] Epoch 1/10 | Batch 300/3572 | Train Loss=0.861338198184967 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 400/3572 | Train Loss=0.6420707702636719 | Train Acc=0.625\n","[TRAIN] Epoch 1/10 | Batch 500/3572 | Train Loss=0.550082266330719 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 600/3572 | Train Loss=0.7165206074714661 | Train Acc=0.625\n","[TRAIN] Epoch 1/10 | Batch 700/3572 | Train Loss=0.47239404916763306 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 800/3572 | Train Loss=0.8052493333816528 | Train Acc=0.625\n","[TRAIN] Epoch 1/10 | Batch 900/3572 | Train Loss=0.33045125007629395 | Train Acc=0.875\n","[TRAIN] Epoch 1/10 | Batch 1000/3572 | Train Loss=1.4888794422149658 | Train Acc=0.375\n","[TRAIN] Epoch 1/10 | Batch 1100/3572 | Train Loss=0.3842264711856842 | Train Acc=0.875\n","[TRAIN] Epoch 1/10 | Batch 1200/3572 | Train Loss=0.4016905426979065 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 1300/3572 | Train Loss=0.5995991826057434 | Train Acc=0.875\n","[TRAIN] Epoch 1/10 | Batch 1400/3572 | Train Loss=0.2959303855895996 | Train Acc=0.875\n","[TRAIN] Epoch 1/10 | Batch 1500/3572 | Train Loss=0.0900101289153099 | Train Acc=1.0\n","[TRAIN] Epoch 1/10 | Batch 1600/3572 | Train Loss=0.06944930553436279 | Train Acc=1.0\n","[TRAIN] Epoch 1/10 | Batch 1700/3572 | Train Loss=0.5705024600028992 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 1800/3572 | Train Loss=0.2230638712644577 | Train Acc=1.0\n","[TRAIN] Epoch 1/10 | Batch 1900/3572 | Train Loss=1.3553926944732666 | Train Acc=0.25\n","[TRAIN] Epoch 1/10 | Batch 2000/3572 | Train Loss=0.9120045900344849 | Train Acc=0.625\n","[TRAIN] Epoch 1/10 | Batch 2100/3572 | Train Loss=0.1990239918231964 | Train Acc=1.0\n","[TRAIN] Epoch 1/10 | Batch 2200/3572 | Train Loss=0.24977368116378784 | Train Acc=0.875\n","[TRAIN] Epoch 1/10 | Batch 2300/3572 | Train Loss=0.7951948642730713 | Train Acc=0.5\n","[TRAIN] Epoch 1/10 | Batch 2400/3572 | Train Loss=0.1643219292163849 | Train Acc=1.0\n","[TRAIN] Epoch 1/10 | Batch 2500/3572 | Train Loss=0.5278522968292236 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 2600/3572 | Train Loss=0.4298529326915741 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 2700/3572 | Train Loss=0.09544121474027634 | Train Acc=1.0\n","[TRAIN] Epoch 1/10 | Batch 2800/3572 | Train Loss=0.6355711221694946 | Train Acc=0.625\n","[TRAIN] Epoch 1/10 | Batch 2900/3572 | Train Loss=0.5747784376144409 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 3000/3572 | Train Loss=0.9513762593269348 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 3100/3572 | Train Loss=0.6483302116394043 | Train Acc=0.75\n","[TRAIN] Epoch 1/10 | Batch 3200/3572 | Train Loss=0.8313984274864197 | Train Acc=0.625\n","[TRAIN] Epoch 1/10 | Batch 3300/3572 | Train Loss=0.9261870384216309 | Train Acc=0.625\n","[TRAIN] Epoch 1/10 | Batch 3400/3572 | Train Loss=0.10285300761461258 | Train Acc=1.0\n","[TRAIN] Epoch 1/10 | Batch 3500/3572 | Train Loss=0.3302502930164337 | Train Acc=0.875\n"," Train Accuracy: 0.7769\n"," Train F1 score: 0.6059\n"," Train Loss: 0.5647\n","Running Validation...\n"," Valid Loss: 0.4964\n"," Valid Accuracy: 0.8112\n"," Valid F1 score: 0.6437\n","Model saved to ==> training_output/model_best_valoss.pt\n","Model saved to ==> training_output/model_best_valacc.pt\n","Model saved to ==> training_output/model_best_valf1.pt\n","Model saved to ==> training_output/model_epoch0.pt\n","======== Epoch 2 / 10 ========\n","Training...\n","[TRAIN] Epoch 2/10 | Batch 0/3572 | Train Loss=0.38632920384407043 | Train Acc=0.875\n","[TRAIN] Epoch 2/10 | Batch 100/3572 | Train Loss=0.2531687319278717 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 200/3572 | Train Loss=0.6988003253936768 | Train Acc=0.875\n","[TRAIN] Epoch 2/10 | Batch 300/3572 | Train Loss=0.747128427028656 | Train Acc=0.625\n","[TRAIN] Epoch 2/10 | Batch 400/3572 | Train Loss=0.5490078926086426 | Train Acc=0.625\n","[TRAIN] Epoch 2/10 | Batch 500/3572 | Train Loss=0.5207124352455139 | Train Acc=0.75\n","[TRAIN] Epoch 2/10 | Batch 600/3572 | Train Loss=1.125923991203308 | Train Acc=0.625\n","[TRAIN] Epoch 2/10 | Batch 700/3572 | Train Loss=0.44284307956695557 | Train Acc=0.75\n","[TRAIN] Epoch 2/10 | Batch 800/3572 | Train Loss=0.5685343146324158 | Train Acc=0.875\n","[TRAIN] Epoch 2/10 | Batch 900/3572 | Train Loss=0.16758805513381958 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 1000/3572 | Train Loss=1.2004241943359375 | Train Acc=0.5\n","[TRAIN] Epoch 2/10 | Batch 1100/3572 | Train Loss=0.2449893355369568 | Train Acc=0.875\n","[TRAIN] Epoch 2/10 | Batch 1200/3572 | Train Loss=0.24590235948562622 | Train Acc=0.875\n","[TRAIN] Epoch 2/10 | Batch 1300/3572 | Train Loss=0.5263131856918335 | Train Acc=0.75\n","[TRAIN] Epoch 2/10 | Batch 1400/3572 | Train Loss=0.20010778307914734 | Train Acc=0.875\n","[TRAIN] Epoch 2/10 | Batch 1500/3572 | Train Loss=0.04145761951804161 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 1600/3572 | Train Loss=0.05290331691503525 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 1700/3572 | Train Loss=0.455890417098999 | Train Acc=0.875\n","[TRAIN] Epoch 2/10 | Batch 1800/3572 | Train Loss=0.13097190856933594 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 1900/3572 | Train Loss=1.2669415473937988 | Train Acc=0.25\n","[TRAIN] Epoch 2/10 | Batch 2000/3572 | Train Loss=0.8674993515014648 | Train Acc=0.75\n","[TRAIN] Epoch 2/10 | Batch 2100/3572 | Train Loss=0.11879070848226547 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 2200/3572 | Train Loss=0.08181987702846527 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 2300/3572 | Train Loss=0.8649617433547974 | Train Acc=0.625\n","[TRAIN] Epoch 2/10 | Batch 2400/3572 | Train Loss=0.10396873205900192 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 2500/3572 | Train Loss=0.171758234500885 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 2600/3572 | Train Loss=0.2795732021331787 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 2700/3572 | Train Loss=0.06727783381938934 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 2800/3572 | Train Loss=0.7372515201568604 | Train Acc=0.5\n","[TRAIN] Epoch 2/10 | Batch 2900/3572 | Train Loss=0.6272998452186584 | Train Acc=0.75\n","[TRAIN] Epoch 2/10 | Batch 3000/3572 | Train Loss=0.9774062633514404 | Train Acc=0.75\n","[TRAIN] Epoch 2/10 | Batch 3100/3572 | Train Loss=0.7442153692245483 | Train Acc=0.625\n","[TRAIN] Epoch 2/10 | Batch 3200/3572 | Train Loss=0.8319498300552368 | Train Acc=0.75\n","[TRAIN] Epoch 2/10 | Batch 3300/3572 | Train Loss=0.1628018319606781 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 3400/3572 | Train Loss=0.09042304009199142 | Train Acc=1.0\n","[TRAIN] Epoch 2/10 | Batch 3500/3572 | Train Loss=0.26687049865722656 | Train Acc=0.875\n"," Train Accuracy: 0.8150\n"," Train F1 score: 0.6706\n"," Train Loss: 0.4851\n","Running Validation...\n"," Valid Loss: 0.4916\n"," Valid Accuracy: 0.8197\n"," Valid F1 score: 0.6652\n","Model saved to ==> training_output/model_best_valoss.pt\n","Model saved to ==> training_output/model_best_valacc.pt\n","Model saved to ==> training_output/model_best_valf1.pt\n","Model saved to ==> training_output/model_epoch1.pt\n","======== Epoch 3 / 10 ========\n","Training...\n","[TRAIN] Epoch 3/10 | Batch 0/3572 | Train Loss=0.13188251852989197 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 100/3572 | Train Loss=0.3216795325279236 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 200/3572 | Train Loss=0.2709318697452545 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 300/3572 | Train Loss=0.5045121908187866 | Train Acc=0.625\n","[TRAIN] Epoch 3/10 | Batch 400/3572 | Train Loss=0.594558596611023 | Train Acc=0.625\n","[TRAIN] Epoch 3/10 | Batch 500/3572 | Train Loss=0.7915467023849487 | Train Acc=0.75\n","[TRAIN] Epoch 3/10 | Batch 600/3572 | Train Loss=1.1089904308319092 | Train Acc=0.625\n","[TRAIN] Epoch 3/10 | Batch 700/3572 | Train Loss=0.510099470615387 | Train Acc=0.75\n","[TRAIN] Epoch 3/10 | Batch 800/3572 | Train Loss=0.38883039355278015 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 900/3572 | Train Loss=0.5375659465789795 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 1000/3572 | Train Loss=1.0936977863311768 | Train Acc=0.625\n","[TRAIN] Epoch 3/10 | Batch 1100/3572 | Train Loss=0.09987438470125198 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 1200/3572 | Train Loss=0.38772520422935486 | Train Acc=0.75\n","[TRAIN] Epoch 3/10 | Batch 1300/3572 | Train Loss=0.22239437699317932 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 1400/3572 | Train Loss=0.13973087072372437 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 1500/3572 | Train Loss=0.024898644536733627 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 1600/3572 | Train Loss=0.04401475563645363 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 1700/3572 | Train Loss=0.21594111621379852 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 1800/3572 | Train Loss=0.12198647856712341 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 1900/3572 | Train Loss=1.462750792503357 | Train Acc=0.375\n","[TRAIN] Epoch 3/10 | Batch 2000/3572 | Train Loss=0.5920425057411194 | Train Acc=0.75\n","[TRAIN] Epoch 3/10 | Batch 2100/3572 | Train Loss=0.043575186282396317 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 2200/3572 | Train Loss=0.05344635620713234 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 2300/3572 | Train Loss=0.3272555470466614 | Train Acc=0.75\n","[TRAIN] Epoch 3/10 | Batch 2400/3572 | Train Loss=0.0697484090924263 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 2500/3572 | Train Loss=0.0841972827911377 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 2600/3572 | Train Loss=0.4294680655002594 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 2700/3572 | Train Loss=0.024798013269901276 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 2800/3572 | Train Loss=0.626189649105072 | Train Acc=0.625\n","[TRAIN] Epoch 3/10 | Batch 2900/3572 | Train Loss=0.6707620024681091 | Train Acc=0.75\n","[TRAIN] Epoch 3/10 | Batch 3000/3572 | Train Loss=1.0908126831054688 | Train Acc=0.75\n","[TRAIN] Epoch 3/10 | Batch 3100/3572 | Train Loss=0.8935574889183044 | Train Acc=0.625\n","[TRAIN] Epoch 3/10 | Batch 3200/3572 | Train Loss=0.8186177611351013 | Train Acc=0.5\n","[TRAIN] Epoch 3/10 | Batch 3300/3572 | Train Loss=0.23147301375865936 | Train Acc=0.875\n","[TRAIN] Epoch 3/10 | Batch 3400/3572 | Train Loss=0.05734225735068321 | Train Acc=1.0\n","[TRAIN] Epoch 3/10 | Batch 3500/3572 | Train Loss=0.28045549988746643 | Train Acc=0.875\n"," Train Accuracy: 0.8322\n"," Train F1 score: 0.7004\n"," Train Loss: 0.4529\n","Running Validation...\n"," Valid Loss: 0.5123\n"," Valid Accuracy: 0.8150\n"," Valid F1 score: 0.6542\n","Model saved to ==> training_output/model_epoch2.pt\n","======== Epoch 4 / 10 ========\n","Training...\n","[TRAIN] Epoch 4/10 | Batch 0/3572 | Train Loss=0.22464370727539062 | Train Acc=0.875\n","[TRAIN] Epoch 4/10 | Batch 100/3572 | Train Loss=0.29786455631256104 | Train Acc=0.875\n","[TRAIN] Epoch 4/10 | Batch 200/3572 | Train Loss=0.05163291096687317 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 300/3572 | Train Loss=0.4874945282936096 | Train Acc=0.75\n","[TRAIN] Epoch 4/10 | Batch 400/3572 | Train Loss=0.5754982233047485 | Train Acc=0.875\n","[TRAIN] Epoch 4/10 | Batch 500/3572 | Train Loss=0.8613224029541016 | Train Acc=0.75\n","[TRAIN] Epoch 4/10 | Batch 600/3572 | Train Loss=0.9864892959594727 | Train Acc=0.625\n","[TRAIN] Epoch 4/10 | Batch 700/3572 | Train Loss=0.7081694006919861 | Train Acc=0.75\n","[TRAIN] Epoch 4/10 | Batch 800/3572 | Train Loss=0.5894455909729004 | Train Acc=0.625\n","[TRAIN] Epoch 4/10 | Batch 900/3572 | Train Loss=0.7637874484062195 | Train Acc=0.875\n","[TRAIN] Epoch 4/10 | Batch 1000/3572 | Train Loss=0.6270256638526917 | Train Acc=0.75\n","[TRAIN] Epoch 4/10 | Batch 1100/3572 | Train Loss=0.3234615921974182 | Train Acc=0.875\n","[TRAIN] Epoch 4/10 | Batch 1200/3572 | Train Loss=0.4637322425842285 | Train Acc=0.75\n","[TRAIN] Epoch 4/10 | Batch 1300/3572 | Train Loss=0.17096863687038422 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 1400/3572 | Train Loss=0.05891970917582512 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 1500/3572 | Train Loss=0.022916410118341446 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 1600/3572 | Train Loss=0.048288021236658096 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 1700/3572 | Train Loss=0.0636216402053833 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 1800/3572 | Train Loss=0.07304924726486206 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 1900/3572 | Train Loss=1.4777402877807617 | Train Acc=0.5\n","[TRAIN] Epoch 4/10 | Batch 2000/3572 | Train Loss=0.7437695860862732 | Train Acc=0.75\n","[TRAIN] Epoch 4/10 | Batch 2100/3572 | Train Loss=0.05237783119082451 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 2200/3572 | Train Loss=0.044804252684116364 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 2300/3572 | Train Loss=1.2468812465667725 | Train Acc=0.625\n","[TRAIN] Epoch 4/10 | Batch 2400/3572 | Train Loss=0.040779441595077515 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 2500/3572 | Train Loss=0.04072638973593712 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 2600/3572 | Train Loss=0.16984505951404572 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 2700/3572 | Train Loss=0.04475972428917885 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 2800/3572 | Train Loss=0.6707454323768616 | Train Acc=0.625\n","[TRAIN] Epoch 4/10 | Batch 2900/3572 | Train Loss=0.48314183950424194 | Train Acc=0.875\n","[TRAIN] Epoch 4/10 | Batch 3000/3572 | Train Loss=1.0071359872817993 | Train Acc=0.75\n","[TRAIN] Epoch 4/10 | Batch 3100/3572 | Train Loss=0.793558657169342 | Train Acc=0.625\n","[TRAIN] Epoch 4/10 | Batch 3200/3572 | Train Loss=1.2813550233840942 | Train Acc=0.625\n","[TRAIN] Epoch 4/10 | Batch 3300/3572 | Train Loss=0.050017159432172775 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 3400/3572 | Train Loss=0.03318840265274048 | Train Acc=1.0\n","[TRAIN] Epoch 4/10 | Batch 3500/3572 | Train Loss=0.03773067891597748 | Train Acc=1.0\n"," Train Accuracy: 0.8505\n"," Train F1 score: 0.7343\n"," Train Loss: 0.4284\n","Running Validation...\n"," Valid Loss: 0.5945\n"," Valid Accuracy: 0.7961\n"," Valid F1 score: 0.6387\n","Model saved to ==> training_output/model_epoch3.pt\n","======== Epoch 5 / 10 ========\n","Training...\n","[TRAIN] Epoch 5/10 | Batch 0/3572 | Train Loss=0.21800914406776428 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 100/3572 | Train Loss=0.37348058819770813 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 200/3572 | Train Loss=0.022056972607970238 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 300/3572 | Train Loss=0.3115118443965912 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 400/3572 | Train Loss=0.31459304690361023 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 500/3572 | Train Loss=0.8264790773391724 | Train Acc=0.75\n","[TRAIN] Epoch 5/10 | Batch 600/3572 | Train Loss=1.5365843772888184 | Train Acc=0.625\n","[TRAIN] Epoch 5/10 | Batch 700/3572 | Train Loss=0.2828410267829895 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 800/3572 | Train Loss=0.2877494990825653 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 900/3572 | Train Loss=0.7964985966682434 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 1000/3572 | Train Loss=0.5852112174034119 | Train Acc=0.625\n","[TRAIN] Epoch 5/10 | Batch 1100/3572 | Train Loss=0.0930243730545044 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 1200/3572 | Train Loss=0.17547783255577087 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 1300/3572 | Train Loss=0.10013064742088318 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 1400/3572 | Train Loss=0.024681100621819496 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 1500/3572 | Train Loss=0.019630854949355125 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 1600/3572 | Train Loss=0.02817625179886818 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 1700/3572 | Train Loss=0.03811038285493851 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 1800/3572 | Train Loss=0.025239693000912666 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 1900/3572 | Train Loss=1.016965627670288 | Train Acc=0.5\n","[TRAIN] Epoch 5/10 | Batch 2000/3572 | Train Loss=0.48330673575401306 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 2100/3572 | Train Loss=0.026957087218761444 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 2200/3572 | Train Loss=0.017900900915265083 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 2300/3572 | Train Loss=0.8874288201332092 | Train Acc=0.625\n","[TRAIN] Epoch 5/10 | Batch 2400/3572 | Train Loss=0.03037000447511673 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 2500/3572 | Train Loss=0.02642247825860977 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 2600/3572 | Train Loss=0.5865271091461182 | Train Acc=0.75\n","[TRAIN] Epoch 5/10 | Batch 2700/3572 | Train Loss=0.01807207241654396 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 2800/3572 | Train Loss=0.7215312719345093 | Train Acc=0.625\n","[TRAIN] Epoch 5/10 | Batch 2900/3572 | Train Loss=0.4413260519504547 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 3000/3572 | Train Loss=1.0273159742355347 | Train Acc=0.75\n","[TRAIN] Epoch 5/10 | Batch 3100/3572 | Train Loss=0.9511905908584595 | Train Acc=0.75\n","[TRAIN] Epoch 5/10 | Batch 3200/3572 | Train Loss=1.1517083644866943 | Train Acc=0.75\n","[TRAIN] Epoch 5/10 | Batch 3300/3572 | Train Loss=0.6416341662406921 | Train Acc=0.875\n","[TRAIN] Epoch 5/10 | Batch 3400/3572 | Train Loss=0.019567085430026054 | Train Acc=1.0\n","[TRAIN] Epoch 5/10 | Batch 3500/3572 | Train Loss=0.016956238076090813 | Train Acc=1.0\n"," Train Accuracy: 0.8683\n"," Train F1 score: 0.7651\n"," Train Loss: 0.4091\n","Running Validation...\n"," Valid Loss: 0.6870\n"," Valid Accuracy: 0.7986\n"," Valid F1 score: 0.6485\n","Model saved to ==> training_output/model_epoch4.pt\n","======== Epoch 6 / 10 ========\n","Training...\n","[TRAIN] Epoch 6/10 | Batch 0/3572 | Train Loss=0.015152320265769958 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 100/3572 | Train Loss=0.27463334798812866 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 200/3572 | Train Loss=0.018434811383485794 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 300/3572 | Train Loss=0.6891902089118958 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 400/3572 | Train Loss=0.17575432360172272 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 500/3572 | Train Loss=0.9394841194152832 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 600/3572 | Train Loss=1.510709524154663 | Train Acc=0.625\n","[TRAIN] Epoch 6/10 | Batch 700/3572 | Train Loss=0.47519782185554504 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 800/3572 | Train Loss=0.384573757648468 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 900/3572 | Train Loss=0.8200727105140686 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 1000/3572 | Train Loss=0.4950059652328491 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 1100/3572 | Train Loss=0.14128810167312622 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 1200/3572 | Train Loss=0.06917975097894669 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 1300/3572 | Train Loss=0.04859934747219086 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 1400/3572 | Train Loss=0.01815914548933506 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 1500/3572 | Train Loss=0.013054885901510715 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 1600/3572 | Train Loss=0.018096398562192917 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 1700/3572 | Train Loss=0.45883750915527344 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 1800/3572 | Train Loss=0.02164211869239807 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 1900/3572 | Train Loss=1.367266058921814 | Train Acc=0.625\n","[TRAIN] Epoch 6/10 | Batch 2000/3572 | Train Loss=0.3836084008216858 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 2100/3572 | Train Loss=0.01597525179386139 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 2200/3572 | Train Loss=0.015618021599948406 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 2300/3572 | Train Loss=1.0031083822250366 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 2400/3572 | Train Loss=0.016372697427868843 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 2500/3572 | Train Loss=0.01919935643672943 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 2600/3572 | Train Loss=0.16360247135162354 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 2700/3572 | Train Loss=0.006572679616510868 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 2800/3572 | Train Loss=0.9264983534812927 | Train Acc=0.625\n","[TRAIN] Epoch 6/10 | Batch 2900/3572 | Train Loss=0.3621009588241577 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 3000/3572 | Train Loss=1.154198169708252 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 3100/3572 | Train Loss=0.7996062636375427 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 3200/3572 | Train Loss=0.8860859274864197 | Train Acc=0.75\n","[TRAIN] Epoch 6/10 | Batch 3300/3572 | Train Loss=0.2803088128566742 | Train Acc=0.875\n","[TRAIN] Epoch 6/10 | Batch 3400/3572 | Train Loss=0.015901798382401466 | Train Acc=1.0\n","[TRAIN] Epoch 6/10 | Batch 3500/3572 | Train Loss=0.01802215725183487 | Train Acc=1.0\n"," Train Accuracy: 0.8803\n"," Train F1 score: 0.7883\n"," Train Loss: 0.3946\n","Running Validation...\n"," Valid Loss: 0.7539\n"," Valid Accuracy: 0.8002\n"," Valid F1 score: 0.6433\n","Model saved to ==> training_output/model_epoch5.pt\n","======== Epoch 7 / 10 ========\n","Training...\n","[TRAIN] Epoch 7/10 | Batch 0/3572 | Train Loss=0.012404976412653923 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 100/3572 | Train Loss=0.8113150596618652 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 200/3572 | Train Loss=0.012474418617784977 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 300/3572 | Train Loss=0.06692826747894287 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 400/3572 | Train Loss=0.5227324366569519 | Train Acc=0.875\n","[TRAIN] Epoch 7/10 | Batch 500/3572 | Train Loss=0.7561354041099548 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 600/3572 | Train Loss=1.2228376865386963 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 700/3572 | Train Loss=0.5855481624603271 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 800/3572 | Train Loss=0.503014087677002 | Train Acc=0.875\n","[TRAIN] Epoch 7/10 | Batch 900/3572 | Train Loss=0.8224237561225891 | Train Acc=0.875\n","[TRAIN] Epoch 7/10 | Batch 1000/3572 | Train Loss=0.31320181488990784 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 1100/3572 | Train Loss=0.015449240803718567 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1200/3572 | Train Loss=0.07594136893749237 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1300/3572 | Train Loss=0.03531276434659958 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1400/3572 | Train Loss=0.016008956357836723 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1500/3572 | Train Loss=0.012697458267211914 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1600/3572 | Train Loss=0.011358707211911678 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1700/3572 | Train Loss=0.024277791380882263 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1800/3572 | Train Loss=0.03965957090258598 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 1900/3572 | Train Loss=0.9389538764953613 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 2000/3572 | Train Loss=0.5816296935081482 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 2100/3572 | Train Loss=0.01799182966351509 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 2200/3572 | Train Loss=0.00940072163939476 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 2300/3572 | Train Loss=0.7086808085441589 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 2400/3572 | Train Loss=0.011308601126074791 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 2500/3572 | Train Loss=0.01793639175593853 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 2600/3572 | Train Loss=0.12738531827926636 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 2700/3572 | Train Loss=0.005061351228505373 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 2800/3572 | Train Loss=0.436492383480072 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 2900/3572 | Train Loss=0.11227817833423615 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 3000/3572 | Train Loss=1.1291292905807495 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 3100/3572 | Train Loss=0.765405535697937 | Train Acc=0.75\n","[TRAIN] Epoch 7/10 | Batch 3200/3572 | Train Loss=0.7419971823692322 | Train Acc=0.875\n","[TRAIN] Epoch 7/10 | Batch 3300/3572 | Train Loss=0.45738059282302856 | Train Acc=0.875\n","[TRAIN] Epoch 7/10 | Batch 3400/3572 | Train Loss=0.020944897085428238 | Train Acc=1.0\n","[TRAIN] Epoch 7/10 | Batch 3500/3572 | Train Loss=0.670124351978302 | Train Acc=0.75\n"," Train Accuracy: 0.8915\n"," Train F1 score: 0.8118\n"," Train Loss: 0.3801\n","Running Validation...\n"," Valid Loss: 0.8679\n"," Valid Accuracy: 0.7967\n"," Valid F1 score: 0.6367\n","Model saved to ==> training_output/model_epoch6.pt\n","======== Epoch 8 / 10 ========\n","Training...\n","[TRAIN] Epoch 8/10 | Batch 0/3572 | Train Loss=0.008184075355529785 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 100/3572 | Train Loss=0.03248509019613266 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 200/3572 | Train Loss=0.006971720140427351 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 300/3572 | Train Loss=0.027858717367053032 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 400/3572 | Train Loss=0.696699857711792 | Train Acc=0.75\n","[TRAIN] Epoch 8/10 | Batch 500/3572 | Train Loss=0.8452480435371399 | Train Acc=0.75\n","[TRAIN] Epoch 8/10 | Batch 600/3572 | Train Loss=1.1881487369537354 | Train Acc=0.75\n","[TRAIN] Epoch 8/10 | Batch 700/3572 | Train Loss=0.551875650882721 | Train Acc=0.75\n","[TRAIN] Epoch 8/10 | Batch 800/3572 | Train Loss=0.2739133834838867 | Train Acc=0.875\n","[TRAIN] Epoch 8/10 | Batch 900/3572 | Train Loss=0.8308039903640747 | Train Acc=0.875\n","[TRAIN] Epoch 8/10 | Batch 1000/3572 | Train Loss=1.8322649002075195 | Train Acc=0.5\n","[TRAIN] Epoch 8/10 | Batch 1100/3572 | Train Loss=0.013636686839163303 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 1200/3572 | Train Loss=0.014689515344798565 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 1300/3572 | Train Loss=0.026872510090470314 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 1400/3572 | Train Loss=0.526735246181488 | Train Acc=0.875\n","[TRAIN] Epoch 8/10 | Batch 1500/3572 | Train Loss=0.00742175942286849 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 1600/3572 | Train Loss=0.01135554350912571 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 1700/3572 | Train Loss=0.010993709787726402 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 1800/3572 | Train Loss=0.010688768699765205 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 1900/3572 | Train Loss=1.4918043613433838 | Train Acc=0.5\n","[TRAIN] Epoch 8/10 | Batch 2000/3572 | Train Loss=0.455110102891922 | Train Acc=0.875\n","[TRAIN] Epoch 8/10 | Batch 2100/3572 | Train Loss=0.008573956787586212 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 2200/3572 | Train Loss=0.00929168239235878 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 2300/3572 | Train Loss=0.8260383605957031 | Train Acc=0.75\n","[TRAIN] Epoch 8/10 | Batch 2400/3572 | Train Loss=0.011608263477683067 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 2500/3572 | Train Loss=0.009150528348982334 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 2600/3572 | Train Loss=0.025327043607831 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 2700/3572 | Train Loss=0.005181280896067619 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 2800/3572 | Train Loss=1.0415064096450806 | Train Acc=0.625\n","[TRAIN] Epoch 8/10 | Batch 2900/3572 | Train Loss=0.06574565917253494 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 3000/3572 | Train Loss=1.2122472524642944 | Train Acc=0.75\n","[TRAIN] Epoch 8/10 | Batch 3100/3572 | Train Loss=1.7957077026367188 | Train Acc=0.625\n","[TRAIN] Epoch 8/10 | Batch 3200/3572 | Train Loss=0.7728880643844604 | Train Acc=0.875\n","[TRAIN] Epoch 8/10 | Batch 3300/3572 | Train Loss=0.031753722578287125 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 3400/3572 | Train Loss=0.01231131237000227 | Train Acc=1.0\n","[TRAIN] Epoch 8/10 | Batch 3500/3572 | Train Loss=0.005962900817394257 | Train Acc=1.0\n"," Train Accuracy: 0.8999\n"," Train F1 score: 0.8276\n"," Train Loss: 0.3686\n","Running Validation...\n"," Valid Loss: 0.8684\n"," Valid Accuracy: 0.8001\n"," Valid F1 score: 0.6466\n","Model saved to ==> training_output/model_epoch7.pt\n","======== Epoch 9 / 10 ========\n","Training...\n","[TRAIN] Epoch 9/10 | Batch 0/3572 | Train Loss=0.1308165341615677 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 100/3572 | Train Loss=0.04237953945994377 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 200/3572 | Train Loss=0.007351393345743418 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 300/3572 | Train Loss=0.05218520760536194 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 400/3572 | Train Loss=0.5405613780021667 | Train Acc=0.75\n","[TRAIN] Epoch 9/10 | Batch 500/3572 | Train Loss=1.0692106485366821 | Train Acc=0.75\n","[TRAIN] Epoch 9/10 | Batch 600/3572 | Train Loss=1.4009007215499878 | Train Acc=0.625\n","[TRAIN] Epoch 9/10 | Batch 700/3572 | Train Loss=0.2163497656583786 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 800/3572 | Train Loss=0.5847911834716797 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 900/3572 | Train Loss=0.8682218790054321 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 1000/3572 | Train Loss=1.799919605255127 | Train Acc=0.625\n","[TRAIN] Epoch 9/10 | Batch 1100/3572 | Train Loss=0.44834327697753906 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 1200/3572 | Train Loss=0.7591710090637207 | Train Acc=0.75\n","[TRAIN] Epoch 9/10 | Batch 1300/3572 | Train Loss=0.02081727236509323 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 1400/3572 | Train Loss=0.009489930234849453 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 1500/3572 | Train Loss=0.005979572888463736 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 1600/3572 | Train Loss=0.0119033707305789 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 1700/3572 | Train Loss=0.011559350416064262 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 1800/3572 | Train Loss=0.016281215474009514 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 1900/3572 | Train Loss=1.1164774894714355 | Train Acc=0.75\n","[TRAIN] Epoch 9/10 | Batch 2000/3572 | Train Loss=0.28075483441352844 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 2100/3572 | Train Loss=0.01300988718867302 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 2200/3572 | Train Loss=0.010778110474348068 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 2300/3572 | Train Loss=0.2017131894826889 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 2400/3572 | Train Loss=0.0564243458211422 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 2500/3572 | Train Loss=0.008523155003786087 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 2600/3572 | Train Loss=0.026463018730282784 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 2700/3572 | Train Loss=0.007078890688717365 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 2800/3572 | Train Loss=0.4589119553565979 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 2900/3572 | Train Loss=0.3998922109603882 | Train Acc=0.875\n","[TRAIN] Epoch 9/10 | Batch 3000/3572 | Train Loss=1.0769717693328857 | Train Acc=0.75\n","[TRAIN] Epoch 9/10 | Batch 3100/3572 | Train Loss=0.9686244130134583 | Train Acc=0.75\n","[TRAIN] Epoch 9/10 | Batch 3200/3572 | Train Loss=1.0723850727081299 | Train Acc=0.75\n","[TRAIN] Epoch 9/10 | Batch 3300/3572 | Train Loss=0.028513845056295395 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 3400/3572 | Train Loss=0.02932005003094673 | Train Acc=1.0\n","[TRAIN] Epoch 9/10 | Batch 3500/3572 | Train Loss=0.011644034646451473 | Train Acc=1.0\n"," Train Accuracy: 0.9089\n"," Train F1 score: 0.8409\n"," Train Loss: 0.3454\n","Running Validation...\n"," Valid Loss: 0.9702\n"," Valid Accuracy: 0.7876\n"," Valid F1 score: 0.6444\n","Model saved to ==> training_output/model_epoch8.pt\n","======== Epoch 10 / 10 ========\n","Training...\n","[TRAIN] Epoch 10/10 | Batch 0/3572 | Train Loss=0.0068754637613892555 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 100/3572 | Train Loss=0.4878598749637604 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 200/3572 | Train Loss=0.007238422520458698 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 300/3572 | Train Loss=0.020963268354535103 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 400/3572 | Train Loss=0.3785611391067505 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 500/3572 | Train Loss=0.5291792154312134 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 600/3572 | Train Loss=1.3970346450805664 | Train Acc=0.75\n","[TRAIN] Epoch 10/10 | Batch 700/3572 | Train Loss=0.2715584933757782 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 800/3572 | Train Loss=0.5873640775680542 | Train Acc=0.75\n","[TRAIN] Epoch 10/10 | Batch 900/3572 | Train Loss=0.9139520525932312 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 1000/3572 | Train Loss=0.5122038722038269 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 1100/3572 | Train Loss=0.008271261118352413 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1200/3572 | Train Loss=0.014605779200792313 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1300/3572 | Train Loss=0.03107551485300064 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1400/3572 | Train Loss=0.008771318942308426 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1500/3572 | Train Loss=0.0072977193631231785 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1600/3572 | Train Loss=0.012096253223717213 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1700/3572 | Train Loss=0.008339513093233109 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1800/3572 | Train Loss=0.019906887784600258 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 1900/3572 | Train Loss=0.7484376430511475 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 2000/3572 | Train Loss=0.9773154854774475 | Train Acc=0.75\n","[TRAIN] Epoch 10/10 | Batch 2100/3572 | Train Loss=0.014939938671886921 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 2200/3572 | Train Loss=0.00843016430735588 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 2300/3572 | Train Loss=0.4571983218193054 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 2400/3572 | Train Loss=0.006206953432410955 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 2500/3572 | Train Loss=0.022413887083530426 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 2600/3572 | Train Loss=0.015535813756287098 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 2700/3572 | Train Loss=0.004636082332581282 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 2800/3572 | Train Loss=0.41035196185112 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 2900/3572 | Train Loss=0.5920805335044861 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 3000/3572 | Train Loss=1.1358866691589355 | Train Acc=0.75\n","[TRAIN] Epoch 10/10 | Batch 3100/3572 | Train Loss=1.451897382736206 | Train Acc=0.75\n","[TRAIN] Epoch 10/10 | Batch 3200/3572 | Train Loss=1.3172982931137085 | Train Acc=0.75\n","[TRAIN] Epoch 10/10 | Batch 3300/3572 | Train Loss=0.3806835412979126 | Train Acc=0.875\n","[TRAIN] Epoch 10/10 | Batch 3400/3572 | Train Loss=0.01011417806148529 | Train Acc=1.0\n","[TRAIN] Epoch 10/10 | Batch 3500/3572 | Train Loss=0.004909525625407696 | Train Acc=1.0\n"," Train Accuracy: 0.9160\n"," Train F1 score: 0.8544\n"," Train Loss: 0.3259\n","Running Validation...\n"," Valid Loss: 0.9399\n"," Valid Accuracy: 0.7980\n"," Valid F1 score: 0.6438\n","Model saved to ==> training_output/model_epoch9.pt\n","Training complete!\n","Best Loss: 0.49157577753067017 (Epoch 2)\n","Best Accuracy: 0.8196787508973438 (Epoch 2)\n","Best F1 Score: 0.6651722323832862 (Epoch 2)\n"]}],"source":["from vncorenlp import VnCoreNLP\n","from keras.preprocessing.sequence import pad_sequences\n","import random\n","from tqdm import tqdm\n","import pickle\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import torch\n","import numpy as np\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n","import os\n","from transformers import RobertaForSequenceClassification, RobertaConfig, AdamW, RobertaTokenizer, RobertaTokenizerFast, RobertaModel, AutoTokenizer\n","from datetime import datetime\n","import glob\n","\n","\n","def make_mask(batch_ids):\n","    batch_mask = []\n","    for ids in batch_ids:\n","        mask = [int(token_id > 0) for token_id in ids]\n","        batch_mask.append(mask)\n","    return torch.tensor(batch_mask)\n","\n","\n","def dataloader_from_text(text_file=None, tokenizer=None, classes=[], savetodisk=None, loadformdisk=None, segment=False,\n","                         max_len=128, batch_size=16, infer=False):\n","    ids_padded, masks, labels = [], [], []\n","    if loadformdisk == None:\n","        # segementer\n","        if segment:\n","            rdrsegmenter = VnCoreNLP(\"./VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","        texts = []\n","        print(\"Loading csv file\")\n","        with open(text_file, 'r', encoding='utf-8') as reading_file:\n","            for sample in tqdm(reading_file):\n","                if infer:\n","                    text = sample.strip()\n","                    if segment:\n","                        text = rdrsegmenter.tokenize(text)\n","                        text = ' '.join([' '.join(x) for x in text])\n","                    texts.append(text)\n","                else:\n","                    splits = sample.strip().split(\",\", -1)\n","                    label = classes.index(splits[len(splits) - 1])\n","                    text = \"\"\n","                    for i in range(len(splits) - 1):\n","                        text += splits[i]\n","                    text = text.replace('\\\"', \"\").replace(\",\", \" ,\").replace(\"?\", \" ?\")\n","                    if segment:\n","                        text = rdrsegmenter.tokenize(text)\n","                        text = ' '.join([' '.join(x) for x in text])\n","                    labels.append(label)\n","                    texts.append(text)\n","\n","        print(\"Encoding text data to input id\")\n","\n","        ids = []\n","        for text in tqdm(texts):\n","            # encode the sentence into list of word id (add bos and eos in the process)\n","            encoded_sent = tokenizer.encode('<s> ' + text + ' </s>')\n","            ids.append(encoded_sent)\n","\n","        del texts\n","        print(\"Padding ids\")\n","        ids_padded = pad_sequences(ids, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","        del ids\n","        # print(\"CREATE MASK\")\n","        # for sent in tqdm(ids_padded):\n","        #     masks.append(make_mask(sent))\n","\n","        masks = []\n","        print('Creating mask')\n","        for sent in ids_padded:\n","            mask = [int(token_id > 0) for token_id in sent]\n","            masks.append(mask)\n","\n","        if savetodisk != None and not infer:\n","            with open(savetodisk, 'wb') as f:\n","                pickle.dump(ids_padded, f)\n","                # pickle.dump(masks, f)\n","                pickle.dump(labels, f)\n","            print(\"Saved padded data to disk\")\n","    else:\n","        print(\"Loadding padded data from disk\")\n","        if loadformdisk != None:\n","            try:\n","                with open(savetodisk, 'rb') as f:\n","                    ids_padded = pickle.load(ids_padded, f)\n","                    # masks = pickle.load(masks, f)\n","                    labels = pickle.load(labels, f)\n","                print(\"Loaded padded data from disk\")\n","            except:\n","                print(\"Error: Loading padded data from disk\")\n","\n","    print(\"Converting to torch tensor\")\n","    ids_inputs = torch.tensor(ids_padded)\n","    del ids_padded\n","    masks_inputs = torch.tensor(masks)\n","    del masks\n","\n","    if not infer:\n","        labels = torch.tensor(labels)\n","\n","    print(\"Create data loader instance\")\n","    if infer:\n","        input_data = TensorDataset(ids_inputs, masks_inputs)\n","        # input_data = TensorDataset(ids_inputs)\n","    else:\n","        input_data = TensorDataset(ids_inputs, masks_inputs, labels)\n","        # input_data = TensorDataset(ids_inputs, labels)\n","    input_sampler = SequentialSampler(input_data)\n","    dataloader = DataLoader(input_data, sampler=input_sampler, batch_size=batch_size)\n","\n","    print(\"len dataloader:\", len(dataloader))\n","    print(\"All data loaded\")\n","    return dataloader\n","\n","# def loss_fn(outputs, targets):\n","#     return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n","\n","class BERTClassifier(torch.nn.Module):\n","    def __init__(self, num_labels):\n","        super(BERTClassifier, self).__init__()\n","        bert_classifier_config = RobertaConfig.from_pretrained(\n","            \"./phobert-base/config.json\",\n","            from_tf=False,\n","            num_labels=num_labels,\n","            output_hidden_states=False,\n","        )\n","        print(\"Loading pre-trained PhoBERT model\")\n","        self.bert_classifier = RobertaForSequenceClassification.from_pretrained(\n","            \"./phobert-base/pytorch_model.bin\",\n","            config=bert_classifier_config\n","        )\n","\n","    def forward(self, input_ids, attention_mask, labels):\n","        output = self.bert_classifier(input_ids=input_ids,\n","                                      token_type_ids=None,\n","                                      attention_mask=attention_mask,\n","                                      labels=labels,\n","                                      )\n","        return output\n","\n","\n","class ClassifierTrainner():\n","    def __init__(self, bert_model, train_dataloader, valid_dataloader, epochs=10, cuda_device=\"cpu\", save_dir=None, use_mode=False):\n","\n","        if cuda_device == \"cpu\":\n","            self.device = torch.device(\"cpu\")\n","        else:\n","            self.device = torch.device('cuda:{}'.format(cuda_device))\n","\n","        self.model = bert_model\n","        self.use_mode = use_mode\n","        if self.use_mode == False:\n","            if save_dir != None and os.path.exists(save_dir):\n","                print(\"Load weight from file:{}\".format(save_dir))\n","                self.save_dir = save_dir\n","                epoch_checkpoint_path = glob.glob(\"{}/model_epoch*\".format(self.save_dir))\n","                if len(epoch_checkpoint_path) == 0:\n","                    print(\"No checkpoint found in: {}\\nCheck save_dir...\".format(self.save_dir))\n","                else:\n","                    self.load_checkpoint(epoch_checkpoint_path)\n","                    print(\"Restore weight successful from: {}\".format(epoch_checkpoint_path))\n","            else:\n","                self.save_dir = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n","                os.makedirs(self.save_dir)\n","                print(\"Training new model, save to: {}\".format(self.save_dir))\n","\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.epochs = epochs\n","        # self.batch_size = batch_size\n","\n","    def save_checkpoint(self, save_path):\n","        state_dict = {'model_state_dict': self.model.state_dict()}\n","        torch.save(state_dict, save_path)\n","        print(f'Model saved to ==> {save_path}')\n","\n","    def load_checkpoint(self, load_path):\n","\n","        # WARNING: device to self.device\n","        state_dict = torch.load(load_path, map_location=self.device)\n","        print(f'Model restored from <== {load_path}')\n","        self.model.load_state_dict(state_dict['model_state_dict'])\n","\n","    @staticmethod\n","    def flat_accuracy(preds, labels):\n","        pred_flat = np.argmax(preds, axis=1).flatten()\n","        labels_flat = labels.flatten()\n","        F1_score = f1_score(pred_flat, labels_flat, average='macro')\n","        return accuracy_score(pred_flat, labels_flat), F1_score\n","\n","    def train_classifier(self):\n","        if self.use_mode == True:\n","            print(\"Model is in use mode, cannot train\")\n","            return\n","        self.model.to(self.device)\n","        param_optimizer = list(self.model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n","\n","        best_valid_loss = 999999\n","        best_valid_loss_rate = 1\n","        best_valid_loss_epoch = -1\n","        best_eval_accuracy = 0\n","        best_eval_accuracy_rate = 0\n","        best_eval_accuracy_epoch = -1\n","        best_f1_score = 0\n","        best_f1_score_rate = 0\n","        best_f1_score_epoch = -1\n","\n","        for epoch_i in range(0, self.epochs):\n","            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, self.epochs))\n","            print('Training...')\n","\n","            # if epoch_i > 0:\n","            #     self.load_checkpoint(\"{}/model_best_valf1.pt\".format(self.save_dir))\n","            total_loss = 0\n","            self.model.train()\n","            train_accuracy = 0\n","            nb_train_steps = 0\n","            train_f1 = 0\n","\n","            for step, batch in enumerate(self.train_dataloader):\n","                b_input_ids = batch[0].to(self.device)\n","                b_input_mask = batch[1].to(self.device)\n","                b_labels = batch[2].to(self.device)\n","\n","                self.model.zero_grad()\n","                outputs = self.model(b_input_ids,\n","                                     # token_type_ids=None,\n","                                     attention_mask=b_input_mask,\n","                                     labels=b_labels\n","                                     )\n","                loss = outputs[0]\n","                total_loss += loss.item()\n","\n","                logits = outputs[1].detach().cpu().numpy()\n","                label_ids = b_labels.cpu().numpy()\n","                tmp_train_accuracy, tmp_train_f1 = self.flat_accuracy(logits, label_ids)\n","                train_accuracy += tmp_train_accuracy\n","                train_f1 += tmp_train_f1\n","                nb_train_steps += 1\n","\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n","                optimizer.step()\n","                if step % 100 == 0:\n","                    print(\n","                        \"[TRAIN] Epoch {}/{} | Batch {}/{} | Train Loss={} | Train Acc={}\".format(epoch_i + 1, self.epochs,\n","                                                                                                  step, len(\n","                                self.train_dataloader), loss.item(), tmp_train_accuracy))\n","\n","            avg_train_loss = total_loss / len(self.train_dataloader)\n","            print(\" Train Accuracy: {0:.4f}\".format(train_accuracy / nb_train_steps))\n","            print(\" Train F1 score: {0:.4f}\".format(train_f1 / nb_train_steps))\n","            print(\" Train Loss: {0:.4f}\".format(avg_train_loss))\n","\n","            print(\"Running Validation...\")\n","            self.model.eval()\n","            eval_loss, eval_accuracy = 0, 0\n","            nb_eval_steps, nb_eval_examples = 0, 0\n","            eval_f1 = 0\n","\n","            for batch in self.valid_dataloader:\n","\n","                batch = tuple(t.to(self.device) for t in batch)\n","                b_input_ids, b_input_mask, b_labels = batch\n","\n","                with torch.no_grad():\n","                    outputs = self.model(b_input_ids,\n","                                         attention_mask=b_input_mask,\n","                                         # token_type_ids=None,\n","                                         labels=b_labels\n","                                         )\n","                    tmp_eval_loss, logits = outputs[0], outputs[1]\n","                    logits = logits.detach().cpu().numpy()\n","                    label_ids = b_labels.cpu().numpy()\n","                    tmp_eval_accuracy, tmp_eval_f1 = self.flat_accuracy(logits, label_ids)\n","                    eval_accuracy += tmp_eval_accuracy\n","                    eval_loss += tmp_eval_loss\n","                    eval_f1 += tmp_eval_f1\n","                    nb_eval_steps += 1\n","\n","            print(\" Valid Loss: {0:.4f}\".format(eval_loss / nb_eval_steps))\n","            print(\" Valid Accuracy: {0:.4f}\".format(eval_accuracy / nb_eval_steps))\n","            print(\" Valid F1 score: {0:.4f}\".format(eval_f1 / nb_eval_steps))\n","\n","            if best_valid_loss > eval_loss:\n","                best_valid_loss = eval_loss\n","                best_valid_loss_rate = eval_loss / nb_eval_steps\n","                best_valid_loss_epoch = epoch_i\n","                best_valid_loss_path = \"{}/model_best_valoss.pt\".format(self.save_dir)\n","                self.save_checkpoint(best_valid_loss_path)\n","            if best_eval_accuracy < eval_accuracy:\n","                best_eval_accuracy = eval_accuracy\n","                best_eval_accuracy_rate = eval_accuracy / nb_eval_steps\n","                best_eval_accuracy_epoch = epoch_i\n","                best_eval_accuracy_path = \"{}/model_best_valacc.pt\".format(self.save_dir)\n","                self.save_checkpoint(best_eval_accuracy_path)\n","            if best_f1_score < eval_f1:\n","                best_f1_score = eval_f1\n","                best_f1_score_rate = eval_f1 / nb_eval_steps\n","                best_f1_score_epoch = epoch_i\n","                best_eval_f1_path = \"{}/model_best_valf1.pt\".format(self.save_dir)\n","                self.save_checkpoint(best_eval_f1_path)\n","\n","            epoch_i_path = \"{}/model_epoch{}.pt\".format(self.save_dir, epoch_i)\n","            self.save_checkpoint(epoch_i_path)\n","            if epoch_i > 0:\n","                os.remove(\"{}/model_epoch{}.pt\".format(self.save_dir, epoch_i - 1))\n","\n","        print(\"Training complete!\")\n","        print(\"Best Loss: {} (Epoch {})\".format(best_valid_loss_rate, best_valid_loss_epoch + 1))\n","        print(\"Best Accuracy: {} (Epoch {})\".format(best_eval_accuracy_rate, best_eval_accuracy_epoch + 1))\n","        print(\"Best F1 Score: {} (Epoch {})\".format(best_f1_score_rate, best_f1_score_epoch + 1))\n","\n","    def predict_dataloader(self, dataloader, classes, tokenizer):\n","        for batch in dataloader:\n","            batch = tuple(t.to(self.device) for t in batch)\n","            b_input_ids, b_input_mask = batch\n","            with torch.no_grad():\n","                outputs = self.model(b_input_ids,\n","                                     attention_mask=b_input_mask,\n","                                     labels=None\n","                                     )\n","                logits = outputs\n","                logits = logits.detach().cpu().numpy()\n","                pred_flat = np.argmax(logits, axis=1).flatten()\n","                print(\"[PREDICT] {}:{}\".format(classes[int(pred_flat)], tokenizer.decode(b_input_ids)))\n","\n","    def predict_text(self, text, classes, tokenizer, max_len=128):\n","\n","        def softmax(x):\n","            \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","            e_x = np.exp(x - np.max(x))\n","            return e_x / e_x.sum(axis=0)  # only difference\n","\n","        ids = tokenizer.encode('<s> ' + text + ' </s>')\n","        ids_padded = pad_sequences([ids], maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","        input_mask = make_mask(ids_padded).to(self.device)\n","        input_ids = torch.tensor(ids_padded)\n","        #input_mask = torch.tensor(mask)\n","        with torch.no_grad():\n","            outputs = self.model(input_ids,\n","                                attention_mask=input_mask,\n","                                labels=None\n","                                )\n","            logits = outputs['logits'].detach().cpu().numpy()\n","            print(softmax(logits[0]))\n","            pred_flat = np.argmax(logits, axis=1).flatten()\n","\n","            return (classes[int(pred_flat)], softmax(logits[0])[int(pred_flat)])\n","#CLASS LIST\n","classes = ['positive', 'neutral', 'negative']\n","\n","\n","def main_train():\n","    MAX_LEN = 256\n","    tokenizer = AutoTokenizer.from_pretrained(\"./phobert-base\", local_files_only=True)\n","\n","    train_path = 'train_ext.csv'\n","    test_path = 'test_ext.csv'\n","\n","    train_dataloader = dataloader_from_text(train_path, tokenizer=tokenizer, classes=classes, savetodisk=None, max_len=MAX_LEN, batch_size=8, segment=True)\n","    valid_dataloader = dataloader_from_text(test_path, tokenizer=tokenizer, classes=classes, savetodisk=None, max_len=MAX_LEN, batch_size=8, segment=True)\n","\n","    #bert model\n","    bert_classifier_model = BERTClassifier(len(classes))\n","    #train model\n","    bert_classifier_trainer = ClassifierTrainner(bert_model=bert_classifier_model, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, epochs=10, cuda_device=\"0\", save_dir='training_output') #cuda_device: \"cpu\"=cpu hoac 0=gpu0, 1=gpu1,\n","    bert_classifier_trainer.train_classifier()\n","    # bert_classifier_trainer.load_checkpoint('09-03-2022_17-44-01/model_best_valf1.pt')\n","    # bert_classifier_trainer.predict_text('Tạm biệt', classes=classes, tokenizer=tokenizer)\n","    # bert_classifier_trainer.predict_text('Xin chào', classes=classes, tokenizer=tokenizer)\n","\n","\n","def evaluate():\n","    # bert model\n","    texts = []\n","    labels = []\n","    MAX_LEN = 256\n","    rdrsegmenter = VnCoreNLP(\"./VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","    tokenizer = AutoTokenizer.from_pretrained(\"./phobert-base\", local_files_only=True)\n","    bert_classifier_model = BERTClassifier(len(classes))\n","    bert_classifier_trainer = ClassifierTrainner(bert_model=bert_classifier_model, train_dataloader=None,\n","                                                 valid_dataloader=None, epochs=10, cuda_device=\"0\", use_mode=True)\n","    bert_classifier_trainer.load_checkpoint('training_output/model_best_valoss.pt')\n","\n","    with open(\"all.csv\", 'r', encoding='utf-8') as reading_file:\n","        for sample in tqdm(reading_file):\n","            splits = sample.strip().split(\",\", 1)\n","            label = splits[1]\n","            text = splits[0]\n","            text = rdrsegmenter.tokenize(text)\n","            text = ' '.join([' '.join(x) for x in text])\n","            labels.append(label)\n","            texts.append(text)\n","\n","    for i in range(len(texts)):\n","        pred_label = bert_classifier_trainer.predict_text(texts[i], classes=classes, tokenizer=tokenizer)\n","        print('{} is {}'.format(texts[i], pred_label) + (\"(Expected to be {})\".format(labels[i]) if pred_label != labels[i] else \"\"))\n","\n","\n","def main():\n","    mode = input(\"Bạn muốn train model mới (y/n): \")\n","    if (mode == \"y\"):\n","        main_train()\n","    else:\n","        #bert model\n","        MAX_LEN = 256\n","        rdrsegmenter = VnCoreNLP(\"./VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","        tokenizer = AutoTokenizer.from_pretrained(\"./phobert-base\", local_files_only=True)\n","        bert_classifier_model = BERTClassifier(len(classes))\n","        bert_classifier_trainer = ClassifierTrainner(bert_model=bert_classifier_model, train_dataloader=None, valid_dataloader=None, epochs=10, cuda_device=\"0\", use_mode=True)\n","        bert_classifier_trainer.load_checkpoint('training_output/model_best_valf1.pt')\n","        quit = False\n","        while (quit == False):\n","            txt = input('Nhập cụm cần phân loại (q để thoát): ')\n","            if (txt != \"q\"):\n","                txt = rdrsegmenter.tokenize(txt)\n","                txt = ' '.join([' '.join(x) for x in txt])\n","                pred_label = bert_classifier_trainer.predict_text(txt, classes=classes, tokenizer=tokenizer)\n","                print('Predicted label for {} is {}'.format(txt, pred_label))\n","            else:\n","                quit = True\n","\n","\n","main()"]},{"cell_type":"markdown","source":["get the model file to my own drive before they get zapped"],"metadata":{"id":"mRzPBN7opEGG"}},{"cell_type":"code","source":["# mount it\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# copy it there\n","# !cp training_output/model_best_valf1.pt /content/drive/MyDrive\n","# !cp training_output/model_best_valoss.pt /content/drive/MyDrive\n","# !cp training_output/model_best_valacc.pt /content/drive/MyDrive\n","!cp training_output/model_epoch9.pt /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cgwk9nm7n69C","executionInfo":{"status":"ok","timestamp":1653014170247,"user_tz":-420,"elapsed":4212,"user":{"displayName":"Dang Nguyen Ngoc","userId":"11023288777081019836"}},"outputId":"55a8a35e-f448-4853-cb9b-000d3389cd2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}